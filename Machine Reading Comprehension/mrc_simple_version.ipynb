{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 基于截断策略的机器阅读理解任务",
   "id": "6f80cdb4908795df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step1 导入相关包",
   "id": "80bebf8d3418a307"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:23:12.443541Z",
     "start_time": "2025-08-16T10:23:12.428537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ],
   "id": "997f2f5af36b39fe",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step2 加载数据集",
   "id": "3e778494a37e4871"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:54:11.287710Z",
     "start_time": "2025-08-16T08:54:02.062272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"cmrc2018\", cache_dir=\"./data\")\n",
    "dataset"
   ],
   "id": "adb95795f071abb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:54:11.335721Z",
     "start_time": "2025-08-16T08:54:11.320719Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[\"train\"][0]",
   "id": "d4658254987bfd51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TRAIN_186_QUERY_0',\n",
       " 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       " 'question': '范廷颂是什么时候被任为主教的？',\n",
       " 'answers': {'text': ['1963年'], 'answer_start': [30]}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step3 数据处理",
   "id": "b52204b2b403919b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:54:12.132638Z",
     "start_time": "2025-08-16T08:54:11.368333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "tokenizer"
   ],
   "id": "7e4804bb20438e9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='hfl/chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:54:12.178649Z",
     "start_time": "2025-08-16T08:54:12.164644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_dataset = dataset[\"train\"].select(range(10))\n",
    "sample_dataset[\"id\"]"
   ],
   "id": "a953ae76fd8e5072",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(['TRAIN_186_QUERY_0', 'TRAIN_186_QUERY_1', 'TRAIN_186_QUERY_2', 'TRAIN_186_QUERY_3', 'TRAIN_186_QUERY_4'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T09:59:34.428078Z",
     "start_time": "2025-08-16T09:59:34.409075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(text=examples[\"question\"],\n",
    "                     text_pair=examples[\"context\"],\n",
    "                     return_offsets_mapping=True,\n",
    "                     max_length=384,\n",
    "                     truncation=\"only_second\",\n",
    "                     padding=\"max_length\")\n",
    "\n",
    "tokenized_examples = sample_dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_examples[\"input_ids\"][0], len(tokenized_examples[\"input_ids\"][0]))"
   ],
   "id": "a78e2cfd18eb9c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 5745, 2455, 7563, 3221, 784, 720, 3198, 952, 6158, 818, 711, 712, 3136, 4638, 8043, 102, 5745, 2455, 7563, 3364, 3322, 8020, 8024, 8021, 8024, 1760, 1399, 924, 4882, 185, 5735, 4449, 8020, 8021, 8024, 3221, 6632, 1298, 5384, 7716, 1921, 712, 3136, 3364, 3322, 511, 9155, 2399, 6158, 818, 711, 712, 3136, 8039, 8431, 2399, 6158, 3091, 1285, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 4415, 8039, 8447, 2399, 6158, 3091, 1285, 711, 2600, 712, 3136, 8024, 1398, 2399, 2399, 2419, 6158, 3091, 1285, 711, 3364, 3322, 8039, 8170, 2399, 123, 3299, 4895, 686, 511, 5745, 2455, 7563, 754, 9915, 2399, 127, 3299, 8115, 3189, 1762, 6632, 1298, 2123, 2398, 4689, 1921, 712, 3136, 1355, 5683, 3136, 1277, 1139, 4495, 8039, 4997, 2399, 3198, 2970, 1358, 5679, 1962, 3136, 5509, 1400, 8024, 6158, 671, 855, 6632, 1298, 4868, 4266, 2372, 1168, 3777, 1079, 5326, 5330, 1071, 2110, 689, 511, 5745, 2455, 7563, 754, 9211, 2399, 1762, 3777, 1079, 1920, 934, 6887, 7368, 2130, 2768, 4868, 2110, 2110, 689, 511, 5745, 2455, 7563, 754, 8594, 2399, 127, 3299, 127, 3189, 1762, 3777, 1079, 4638, 712, 3136, 2429, 1828, 3232, 7195, 8039, 1350, 1400, 6158, 3836, 1168, 1760, 1957, 2207, 2548, 1065, 2109, 1036, 7368, 3302, 1218, 511, 8707, 2399, 807, 8024, 5745, 2455, 7563, 1762, 3777, 1079, 1828, 1277, 1158, 2456, 4919, 3696, 2970, 2521, 704, 2552, 809, 3119, 2159, 1168, 3777, 1079, 6912, 2773, 4638, 7410, 3696, 511, 9258, 2399, 8024, 3791, 6632, 2773, 751, 5310, 3338, 8024, 6632, 1298, 3696, 712, 1066, 1469, 1744, 2456, 6963, 3777, 1079, 8024, 2496, 3198, 2523, 1914, 1921, 712, 3136, 4868, 5466, 782, 1447, 6845, 5635, 6632, 1298, 4638, 1298, 3175, 8024, 852, 5745, 2455, 7563, 793, 4197, 4522, 1762, 3777, 1079, 511, 5422, 2399, 5052, 4415, 1760, 5735, 3307, 2207, 934, 7368, 8039, 2668, 1762, 8779, 2399, 1728, 2932, 1310, 934, 7368, 4638, 5632, 4507, 510, 5632, 3780, 1350, 2867, 5318, 3124, 2424, 1762, 934, 7368, 6392, 3124, 3780, 6440, 4638, 6206, 3724, 5445, 6158, 2936, 511, 9155, 2399, 125, 3299, 126, 3189, 8024, 3136, 2134, 818, 1462, 5745, 2455, 7563, 711, 1921, 712, 3136, 1266, 2123, 3136, 1277, 712, 3136, 8024, 1398, 2399, 129, 3299, 8115, 3189, 2218, 818, 8039, 1071, 4288, 7208, 711, 519, 2769, 928, 102] 384\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T09:59:35.131699Z",
     "start_time": "2025-08-16T09:59:35.122707Z"
    }
   },
   "cell_type": "code",
   "source": "print(list(zip(tokenized_examples[\"input_ids\"][0], tokenized_examples[\"token_type_ids\"][0])))",
   "id": "ae32e59e0f2cc26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(101, 0), (5745, 0), (2455, 0), (7563, 0), (3221, 0), (784, 0), (720, 0), (3198, 0), (952, 0), (6158, 0), (818, 0), (711, 0), (712, 0), (3136, 0), (4638, 0), (8043, 0), (102, 0), (5745, 1), (2455, 1), (7563, 1), (3364, 1), (3322, 1), (8020, 1), (8024, 1), (8021, 1), (8024, 1), (1760, 1), (1399, 1), (924, 1), (4882, 1), (185, 1), (5735, 1), (4449, 1), (8020, 1), (8021, 1), (8024, 1), (3221, 1), (6632, 1), (1298, 1), (5384, 1), (7716, 1), (1921, 1), (712, 1), (3136, 1), (3364, 1), (3322, 1), (511, 1), (9155, 1), (2399, 1), (6158, 1), (818, 1), (711, 1), (712, 1), (3136, 1), (8039, 1), (8431, 1), (2399, 1), (6158, 1), (3091, 1), (1285, 1), (711, 1), (1921, 1), (712, 1), (3136, 1), (3777, 1), (1079, 1), (2600, 1), (3136, 1), (1277, 1), (2134, 1), (2429, 1), (5392, 1), (4415, 1), (8039, 1), (8447, 1), (2399, 1), (6158, 1), (3091, 1), (1285, 1), (711, 1), (2600, 1), (712, 1), (3136, 1), (8024, 1), (1398, 1), (2399, 1), (2399, 1), (2419, 1), (6158, 1), (3091, 1), (1285, 1), (711, 1), (3364, 1), (3322, 1), (8039, 1), (8170, 1), (2399, 1), (123, 1), (3299, 1), (4895, 1), (686, 1), (511, 1), (5745, 1), (2455, 1), (7563, 1), (754, 1), (9915, 1), (2399, 1), (127, 1), (3299, 1), (8115, 1), (3189, 1), (1762, 1), (6632, 1), (1298, 1), (2123, 1), (2398, 1), (4689, 1), (1921, 1), (712, 1), (3136, 1), (1355, 1), (5683, 1), (3136, 1), (1277, 1), (1139, 1), (4495, 1), (8039, 1), (4997, 1), (2399, 1), (3198, 1), (2970, 1), (1358, 1), (5679, 1), (1962, 1), (3136, 1), (5509, 1), (1400, 1), (8024, 1), (6158, 1), (671, 1), (855, 1), (6632, 1), (1298, 1), (4868, 1), (4266, 1), (2372, 1), (1168, 1), (3777, 1), (1079, 1), (5326, 1), (5330, 1), (1071, 1), (2110, 1), (689, 1), (511, 1), (5745, 1), (2455, 1), (7563, 1), (754, 1), (9211, 1), (2399, 1), (1762, 1), (3777, 1), (1079, 1), (1920, 1), (934, 1), (6887, 1), (7368, 1), (2130, 1), (2768, 1), (4868, 1), (2110, 1), (2110, 1), (689, 1), (511, 1), (5745, 1), (2455, 1), (7563, 1), (754, 1), (8594, 1), (2399, 1), (127, 1), (3299, 1), (127, 1), (3189, 1), (1762, 1), (3777, 1), (1079, 1), (4638, 1), (712, 1), (3136, 1), (2429, 1), (1828, 1), (3232, 1), (7195, 1), (8039, 1), (1350, 1), (1400, 1), (6158, 1), (3836, 1), (1168, 1), (1760, 1), (1957, 1), (2207, 1), (2548, 1), (1065, 1), (2109, 1), (1036, 1), (7368, 1), (3302, 1), (1218, 1), (511, 1), (8707, 1), (2399, 1), (807, 1), (8024, 1), (5745, 1), (2455, 1), (7563, 1), (1762, 1), (3777, 1), (1079, 1), (1828, 1), (1277, 1), (1158, 1), (2456, 1), (4919, 1), (3696, 1), (2970, 1), (2521, 1), (704, 1), (2552, 1), (809, 1), (3119, 1), (2159, 1), (1168, 1), (3777, 1), (1079, 1), (6912, 1), (2773, 1), (4638, 1), (7410, 1), (3696, 1), (511, 1), (9258, 1), (2399, 1), (8024, 1), (3791, 1), (6632, 1), (2773, 1), (751, 1), (5310, 1), (3338, 1), (8024, 1), (6632, 1), (1298, 1), (3696, 1), (712, 1), (1066, 1), (1469, 1), (1744, 1), (2456, 1), (6963, 1), (3777, 1), (1079, 1), (8024, 1), (2496, 1), (3198, 1), (2523, 1), (1914, 1), (1921, 1), (712, 1), (3136, 1), (4868, 1), (5466, 1), (782, 1), (1447, 1), (6845, 1), (5635, 1), (6632, 1), (1298, 1), (4638, 1), (1298, 1), (3175, 1), (8024, 1), (852, 1), (5745, 1), (2455, 1), (7563, 1), (793, 1), (4197, 1), (4522, 1), (1762, 1), (3777, 1), (1079, 1), (511, 1), (5422, 1), (2399, 1), (5052, 1), (4415, 1), (1760, 1), (5735, 1), (3307, 1), (2207, 1), (934, 1), (7368, 1), (8039, 1), (2668, 1), (1762, 1), (8779, 1), (2399, 1), (1728, 1), (2932, 1), (1310, 1), (934, 1), (7368, 1), (4638, 1), (5632, 1), (4507, 1), (510, 1), (5632, 1), (3780, 1), (1350, 1), (2867, 1), (5318, 1), (3124, 1), (2424, 1), (1762, 1), (934, 1), (7368, 1), (6392, 1), (3124, 1), (3780, 1), (6440, 1), (4638, 1), (6206, 1), (3724, 1), (5445, 1), (6158, 1), (2936, 1), (511, 1), (9155, 1), (2399, 1), (125, 1), (3299, 1), (126, 1), (3189, 1), (8024, 1), (3136, 1), (2134, 1), (818, 1), (1462, 1), (5745, 1), (2455, 1), (7563, 1), (711, 1), (1921, 1), (712, 1), (3136, 1), (1266, 1), (2123, 1), (3136, 1), (1277, 1), (712, 1), (3136, 1), (8024, 1), (1398, 1), (2399, 1), (129, 1), (3299, 1), (8115, 1), (3189, 1), (2218, 1), (818, 1), (8039, 1), (1071, 1), (4288, 1), (7208, 1), (711, 1), (519, 1), (2769, 1), (928, 1), (102, 1)]\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T09:59:36.664697Z",
     "start_time": "2025-08-16T09:59:36.651694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "\n",
    "tokenized_examples = tokenized_examples.remove_columns(\"offset_mapping\")\n"
   ],
   "id": "2faf25d2ab194db7",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T09:56:08.902887Z",
     "start_time": "2025-08-16T09:56:08.878882Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenized_examples.sequence_ids(0))",
   "id": "c3b2ad711e2af9a5",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'sequence_ids'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[84], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtokenized_examples\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequence_ids\u001B[49m(\u001B[38;5;241m0\u001B[39m))\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Dataset' object has no attribute 'sequence_ids'"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T09:48:15.816447Z",
     "start_time": "2025-08-16T09:48:15.776266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for idx, offset in enumerate(offset_mapping):\n",
    "    answer = sample_dataset[idx][\"answers\"]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "    context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "    context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "\n",
    "    if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "        start_token_pos = 0\n",
    "        end_token_pos = 0\n",
    "    else:\n",
    "        token_id = context_start\n",
    "        while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "            token_id += 1\n",
    "        start_token_pos = token_id\n",
    "        token_id = context_end\n",
    "        while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "            token_id -= 1\n",
    "        end_token_pos = token_id\n",
    "    print(answer, start_char, end_char, context_start, context_end, start_token_pos, end_token_pos)"
   ],
   "id": "1164da53b55868da",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'sequence_ids'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[77], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m start_char \u001B[38;5;241m=\u001B[39m answer[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer_start\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      4\u001B[0m end_char \u001B[38;5;241m=\u001B[39m start_char \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(answer[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m----> 6\u001B[0m context_start \u001B[38;5;241m=\u001B[39m \u001B[43mtokenized_examples\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequence_ids\u001B[49m(idx)\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      7\u001B[0m context_end \u001B[38;5;241m=\u001B[39m tokenized_examples\u001B[38;5;241m.\u001B[39msequence_ids(idx)\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;28;01mNone\u001B[39;00m, context_start) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m offset[context_end][\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m<\u001B[39m start_char \u001B[38;5;129;01mor\u001B[39;00m offset[context_start][\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m end_char:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Dataset' object has no attribute 'sequence_ids'"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:17:32.361360Z",
     "start_time": "2025-08-16T10:17:32.349357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_fun(examples):\n",
    "    tokenized_examples = tokenizer(text=examples[\"question\"],\n",
    "                         text_pair=examples[\"context\"],\n",
    "                         return_offsets_mapping=True,\n",
    "                         max_length=384,\n",
    "                         truncation=\"only_second\",\n",
    "                         padding=\"max_length\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for idx, offset in enumerate(offset_mapping):\n",
    "        answer = examples[\"answers\"][idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -= 1\n",
    "            end_token_pos = token_id\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples"
   ],
   "id": "9aee473214da874d",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:17:39.699704Z",
     "start_time": "2025-08-16T10:17:32.701503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = dataset.map(process_fun, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_datasets"
   ],
   "id": "346d37b1295a567e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10142 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1626c7d139d4301ae3d4c88d8235227"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3219 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5ea02d686df44588c23b4d82afdc24a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "379cae49bb1b4c49a5b1fec3537bfd35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step4 加载模型",
   "id": "133ecde743c2611a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:19:12.223096Z",
     "start_time": "2025-08-16T10:19:10.540820Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-base\")",
   "id": "c73cc047cc12cda4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step5 配置训练参数",
   "id": "bb87c8ac0187a0c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:24:09.479673Z",
     "start_time": "2025-08-16T10:24:09.403942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(output_dir=\"./models_for_qa\",\n",
    "                         per_device_train_batch_size=16,\n",
    "                         per_device_eval_batch_size=16,\n",
    "                         eval_strategy=\"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         logging_steps=50,\n",
    "                         num_train_epochs=1)"
   ],
   "id": "705cf1388c2cbde8",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step7 创建训练器",
   "id": "7d2bf9829c57ec87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:24:10.189577Z",
     "start_time": "2025-08-16T10:24:10.168573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=DefaultDataCollator()\n",
    ")"
   ],
   "id": "fd37a445e0d1f90d",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step7 模型训练",
   "id": "2fac296c192ca740"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:32:29.942236Z",
     "start_time": "2025-08-16T10:24:18.976479Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "1d09cc229df100ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='634' max='634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [634/634 08:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.312600</td>\n",
       "      <td>1.063297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=634, training_loss=1.6697771752294306, metrics={'train_runtime': 490.7086, 'train_samples_per_second': 20.668, 'train_steps_per_second': 1.292, 'total_flos': 1987553780112384.0, 'train_loss': 1.6697771752294306, 'epoch': 1.0})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step8 模型评估",
   "id": "501ad4c5de067b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:32:30.114800Z",
     "start_time": "2025-08-16T10:32:30.006480Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import pipeline",
   "id": "3849b8f10e362973",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T10:32:38.646112Z",
     "start_time": "2025-08-16T10:32:38.339282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "pipe(question=\"小明在哪里上班\", context=\"小明在北京的一家公司上班。\")"
   ],
   "id": "382eda687a983602",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.3932684063911438, 'start': 3, 'end': 10, 'answer': '北京的一家公司'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77bd092e8433d55f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
